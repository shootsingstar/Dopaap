{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAekAAABlCAYAAABpyxuAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAADhElEQVR4nO3cQU4bdxjG4c+EMGpRbansLANSu+guUtWz9AA9CbDiGD1Aj5AeouoR8B4ZLFoNKUwXrbIIuNgm6bwTnmdn+y/06QPx09gjj7qu6woAiLPT9wAAwONEGgBCiTQAhBJpAAgl0gAQSqQBIJRIA0Co3XUPtm1bbdu+f3x/f1+Xl5d1cHBQo9HokwwHAJ+jrutquVzWdDqtnZ3V18trR/r8/LzOzs4+ynAAQNV8Pq/ZbLby9dG63zj24ZX01dVVHR0d1fyn72u89+r5k74Qlz/+0PcIg/T2zZu+Rxict/Vd3yMM0q+/r/6HyeO++eXrvkcYnLvbZf3287e1WCxqMpmsPLf2lXTTNNU0zYPnx3uvary39o958d7tP9whT/ty/EXfIwzO69rve4RBGu1/1fcIg7O7N+57hMF66uNiN44BQCiRBoBQIg0AoUQaAEKJNACEEmkACCXSABBKpAEglEgDQCiRBoBQIg0AoUQaAEKJNACEEmkACCXSABBKpAEglEgDQCiRBoBQIg0AoUQaAEKJNACEEmkACCXSABBKpAEglEgDQCiRBoBQIg0AoUQaAEKJNACEEmkACCXSABBKpAEglEgDQCiRBoBQIg0AoUQaAEKJNACEEmkACCXSABBKpAEglEgDQCiRBoBQIg0AoUQaAEKJNACEEmkACCXSABBKpAEglEgDQCiRBoBQIg0AoUQaAEKJNACEEmkACCXSABBKpAEglEgDQCiRBoBQIg0AoUQaAEKJNACEEmkACCXSABBKpAEglEgDQCiRBoBQIg0AoUQaAEKJNACEEmkACCXSABBKpAEglEgDQKjddQ+2bVtt275/fHV1VVVV17d3H3+qz9jypn36EA/8cf1n3yMMzru66XuEQepuln2PMDh/3b7ue4TBubv95++s67r/PDfqnjrxr9PT0zo7O3v+ZABAVVXN5/OazWYrX1870h9eSS8Wizo+Pq6Li4uaTCbPn/QFuL6+rsPDw5rP5zUej/seZzDsbXN2th1725ydbafruloulzWdTmtnZ/Unz2u/3d00TTVN8+D5yWTiF7Oh8XhsZ1uwt83Z2XbsbXN2trl1LnDdOAYAoUQaAEJtHemmaerk5OTRt8B5nJ1tx942Z2fbsbfN2dmntfaNYwDA/8vb3QAQSqQBIJRIA0AokQaAUCINAKFEGgBCiTQAhBJpAAj1N0AipuNbVmn1AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 600x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "import molmap.model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette(\"rainbow_r\", 6) #PiYG\n",
    "sns.palplot(color)\n",
    "\n",
    "from joblib import load, dump\n",
    "import sklearn.metrics\n",
    "import scipy\n",
    "import logging\n",
    "\n",
    "def r2_score(y_true,y_pred):\n",
    "    y_mean = np.mean(y_true)\n",
    "    r2 = 1-sum((y_true-y_pred)**2)/sum((y_mean-y_true)**2)\n",
    "    return r2\n",
    "def PCC(y_pred,y_true):\n",
    "    diff_pred,diff_true=y_pred-np.mean(y_pred),y_true-np.mean(y_true)\n",
    "    return np.sum(diff_pred*diff_true)/np.sqrt(np.sum(diff_pred**2)*np.sum(diff_true**2))\n",
    "\n",
    "def from_ic50(ic50, max_ic50=50000.0):\n",
    "    x = 1.0 - (np.log(np.maximum(ic50, 1e-12)) / np.log(max_ic50))\n",
    "    return np.minimum(\n",
    "        1.0,\n",
    "        np.maximum(0.0, x))\n",
    "def to_ic50(x, max_ic50=50000.0):\n",
    "    return max_ic50 ** (1.0 - x)\n",
    "sample_weight=None,\n",
    "threshold_nm=500,\n",
    "max_ic50=50000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取 blosum62 matrix + pam250 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "blosum62 = pd.read_csv('/mnt/zt/Dopaap/dopaap_predictor/blosum_pam_data/BLOSUM62.txt', sep='\\s')\n",
    "blosum62 = blosum62.iloc[:-4,:-4]\n",
    "\n",
    "pam250 = pd.read_csv('/mnt/zt/Dopaap/dopaap_predictor/blosum_pam_data/PAM250.csv',index_col=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 blosum62 matrix 以及 pam250 mutation matrix 标准化到 0~1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aapro归一化\n",
    "PCP = pd.read_csv('/mnt/zt/Dopaap/dopaap_predictor/cal_CTD/CTD_61.csv', index_col='properties').index\n",
    "AApro_dic = pd.read_csv('/mnt/zt/Dopaap/dopaap_predictor/umap/single_index/AApro_UMAP.csv', header=0,index_col=0)\n",
    "AA_pro = AApro_dic.loc[PCP].astype('float')\n",
    "AA_pro = AA_pro.T\n",
    "AA_pro = ((AA_pro - AA_pro.min()) / (AA_pro.max() - AA_pro.min())).T #归一化"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "序列相关 aapro,blosum62,pam250 距离矩阵的生成与拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_3d_feat(seq):\n",
    "    n = len(seq)\n",
    "\n",
    "    # 生成理化性质矩阵\n",
    "    seq_pro = pd.DataFrame(AA_pro[aa] for aa in seq).T\n",
    "    seq_pro = seq_pro.values[:, :, None]  #二维变成三维（61， n， 1）\n",
    "    ## 相乘\n",
    "    mt_pro = np.transpose((seq_pro * np.transpose(seq_pro, [0, 2, 1])), [1, 2, 0])  #**.5   #here!! **0.25\n",
    "    ## transpose三维转置。（61，n，1）*（61，1，n）=（61，n，n），再转置成（n，n，61）.\n",
    "\n",
    "    # 生成blosum矩阵\n",
    "    seq_blosum = np.ones((n,n))\n",
    "    for i in range(n):\n",
    "        a = seq[i]\n",
    "        for j in range(n):\n",
    "            b = seq[j]\n",
    "            seq_blosum[i][j] = blosum62.loc[a,b]\n",
    "    seq_blosum_nor = ((seq_blosum - seq_blosum.min()) / (seq_blosum.max() - seq_blosum.min()))\n",
    "    seq_blosum_nor = seq_blosum_nor[:, :, np.newaxis]  ##（n，n，1）\n",
    "\n",
    "    # 生成pam矩阵\n",
    "    seq_pam = np.ones((n,n))\n",
    "    for i in range(n):\n",
    "        a = seq[i]\n",
    "        for j in range(n):\n",
    "            b = seq[j]\n",
    "            seq_pam[i][j] = pam250.loc[a,b]\n",
    "    seq_pam_nor = ((seq_pam - seq_pam.min()) / (seq_pam.max() - seq_pam.min()))\n",
    "    seq_pam_nor = seq_pam_nor[:, :, np.newaxis]  ##（n，n，1）\n",
    "\n",
    "    # 生成序列距离矩阵\n",
    "    pt_dis = np.ones((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            pt_dis[i][j] = abs(i-j)\n",
    "    pt_dis = ((pt_dis - 0) / (n-1 - 0)) ## 归一化（最小值为0，最大值为n-1）\n",
    "    pt_dis = pt_dis[:, :, np.newaxis]  ##（n，n，1）\n",
    "\n",
    "    # 改变 理化性质矩阵 的下半部分\n",
    "    for k in range(mt_pro.shape[2]):\n",
    "        for i in range(n):\n",
    "            for j in range(i):\n",
    "                if k < 60:                                      ##here\n",
    "                    mt_pro[i,j,k] = (mt_pro[j,i,k] * mt_pro[j,i,k+1])**.5\n",
    "                else:\n",
    "                    mt_pro[i,j,k] = (mt_pro[j,i,k] * mt_pro[j,i,0])**.5\n",
    "\n",
    "    # 将序列距离矩阵与mt合并，并放在第一层\n",
    "    mt = np.concatenate((pt_dis,seq_blosum_nor,seq_pam_nor,mt_pro),axis = 2)\n",
    "\n",
    "    x = np.pad(mt, [(0, max_seq_len-n), (0, max_seq_len-n), (0, 0)]) #填充0.令数据集中每条序列对应的Xshape相同。\n",
    "\n",
    "    return x[ :, :, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units = 8, strides = 1):\n",
    "        super(Inception, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv3D(units, (1,1,1), padding='same', activation = 'relu', strides = strides,kernel_regularizer=tf.keras.regularizers.l2(lamda),\n",
    "  bias_regularizer=tf.keras.regularizers.l1(lamda), activity_regularizer=tf.keras.regularizers.l2(lamda))\n",
    "        self.conv2 = tf.keras.layers.Conv3D(units, (3,3,3), padding='same', activation = 'relu', strides = strides,kernel_regularizer=tf.keras.regularizers.l2(lamda),\n",
    "  bias_regularizer=tf.keras.regularizers.l1(lamda), activity_regularizer=tf.keras.regularizers.l2(lamda))\n",
    "        self.conv3 = tf.keras.layers.Conv3D(units, (5,5,5), padding='same', activation = 'relu', strides = strides,kernel_regularizer=tf.keras.regularizers.l2(lamda),\n",
    "  bias_regularizer=tf.keras.regularizers.l1(lamda), activity_regularizer=tf.keras.regularizers.l2(lamda))\n",
    "        self.concat = tf.keras.layers.Concatenate()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x1 = self.conv1(inputs)\n",
    "        x2 = self.conv2(inputs)\n",
    "        x3 = self.conv3(inputs)\n",
    "        outputs = self.concat([x1, x2, x3])\n",
    "        return outputs\n",
    "\n",
    "    def get_config(self): \n",
    "        config = {\"conv1\": self.conv1,\"conv2\":self.conv2,'conv3':self.conv3}\n",
    "        base_config = super(Inception, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_1(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "            super().__init__()\n",
    "            \n",
    "            self.Cov_1 = tf.keras.layers.Conv3D(kn_1, ks_fir, activation='relu', padding='same',kernel_regularizer=tf.keras.regularizers.l2(lamda))\n",
    "            self.MaxPool_1 = tf.keras.layers.MaxPool3D() #pool_size = (2,2,2)\n",
    "    #         #here\n",
    "    #         tf.keras.layers.Conv3D(kn_1, (5,5,5), activation='relu', padding='same',kernel_regularizer=tf.keras.regularizers.l2(lamda),\n",
    "    # bias_regularizer=tf.keras.regularizers.l1(lamda), activity_regularizer=tf.keras.regularizers.l2(lamda)),\n",
    "    #         tf.keras.layers.MaxPool3D(), #pool_size = (2,2,2)\n",
    "    #         #here\n",
    "            self.Cov_2 =Inception(units=kn_2, strides=1)\n",
    "            self.MaxPool_2 = tf.keras.layers.MaxPool3D()\n",
    "            self.Cov_3 = Inception(units=kn_3, strides=1)\n",
    "            self.GlobalMaxPool = tf.keras.layers.GlobalMaxPooling3D()         #tf.keras.layers.Flatten(),\n",
    "            self.Dense_1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "            self.Dense_2 =tf.keras.layers.Dense(32, activation='relu')\n",
    "            self.Dense_3 =tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, input):\n",
    "            x = self.Cov_1(input)\n",
    "            x = self.MaxPool_1(x)\n",
    "            x = self.Cov_2(x) \n",
    "            x = self.MaxPool_2(x)\n",
    "            x = self.Cov_3(x)\n",
    "            x = self.GlobalMaxPool(x)\n",
    "            x = self.Dense_1(x)\n",
    "            x = self.Dense_2(x)\n",
    "            x = self.Dense_3(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_2(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "            super().__init__()\n",
    "            \n",
    "            self.Cov_1 = tf.keras.layers.Conv3D(kn_1, ks_fir, activation='relu', padding='same',kernel_regularizer=tf.keras.regularizers.l2(lamda))\n",
    "            self.MaxPool_1 = tf.keras.layers.MaxPool3D() #pool_size = (2,2,2)\n",
    "    #         #here\n",
    "    #         tf.keras.layers.Conv3D(kn_1, (5,5,5), activation='relu', padding='same',kernel_regularizer=tf.keras.regularizers.l2(lamda),\n",
    "    # bias_regularizer=tf.keras.regularizers.l1(lamda), activity_regularizer=tf.keras.regularizers.l2(lamda)),\n",
    "    #         tf.keras.layers.MaxPool3D(), #pool_size = (2,2,2)\n",
    "    #         #here\n",
    "            self.Cov_2 =Inception(units=kn_2, strides=1)\n",
    "            self.MaxPool_2 = tf.keras.layers.MaxPool3D()\n",
    "            self.Cov_3 = Inception(units=kn_3, strides=1)\n",
    "            self.GlobalMaxPool = tf.keras.layers.GlobalMaxPooling3D()         #tf.keras.layers.Flatten(),\n",
    "            self.Dense_1 = tf.keras.layers.Dense(512, activation='relu')\n",
    "            self.Dense_2 =tf.keras.layers.Dense(64, activation='relu')\n",
    "            self.Dense_3 =tf.keras.layers.Dense(1)\n",
    "            \n",
    "    \n",
    "    def call(self, input):\n",
    "            x = self.Cov_1(input)\n",
    "            x = self.MaxPool_1(x)\n",
    "            x = self.Cov_2(x) \n",
    "            x = self.MaxPool_2(x)\n",
    "            x = self.Cov_3(x)\n",
    "            x = self.GlobalMaxPool(x)\n",
    "            x = self.Dense_1(x)\n",
    "            x = self.Dense_2(x)\n",
    "            x = self.Dense_3(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_3(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "            super().__init__()\n",
    "            \n",
    "            self.Cov_1 = tf.keras.layers.Conv3D(kn_1, ks_fir, activation='relu', padding='same',kernel_regularizer=tf.keras.regularizers.l2(lamda))\n",
    "            self.MaxPool_1 = tf.keras.layers.MaxPool3D() #pool_size = (2,2,2)\n",
    "    #         #here\n",
    "    #         tf.keras.layers.Conv3D(kn_1, (5,5,5), activation='relu', padding='same',kernel_regularizer=tf.keras.regularizers.l2(lamda),\n",
    "    # bias_regularizer=tf.keras.regularizers.l1(lamda), activity_regularizer=tf.keras.regularizers.l2(lamda)),\n",
    "    #         tf.keras.layers.MaxPool3D(), #pool_size = (2,2,2)\n",
    "    #         #here\n",
    "            self.Cov_2 =Inception(units=kn_2, strides=1)\n",
    "            # self.MaxPool_2 = tf.keras.layers.MaxPool3D()\n",
    "            # self.Cov_3 = Inception(units=kn_3, strides=1)\n",
    "            self.GlobalMaxPool = tf.keras.layers.GlobalMaxPooling3D()         #tf.keras.layers.Flatten(),\n",
    "            self.Dense_1 = tf.keras.layers.Dense(512, activation='relu')\n",
    "            self.Dense_2 =tf.keras.layers.Dense(64, activation='relu')\n",
    "            self.Dense_3 =tf.keras.layers.Dense(1)\n",
    "            \n",
    "    \n",
    "    def call(self, input):\n",
    "            x = self.Cov_1(input)\n",
    "            x = self.MaxPool_1(x)\n",
    "            x = self.Cov_2(x) \n",
    "            # x = self.MaxPool_2(x)\n",
    "            # x = self.Cov_3(x)\n",
    "            x = self.GlobalMaxPool(x)\n",
    "            x = self.Dense_1(x)\n",
    "            x = self.Dense_2(x)\n",
    "            x = self.Dense_3(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_4(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "            super().__init__()\n",
    "            \n",
    "            self.Cov_1 = tf.keras.layers.Conv3D(kn_1, ks_fir, activation='relu', padding='same',kernel_regularizer=tf.keras.regularizers.l2(lamda))\n",
    "            self.MaxPool_1 = tf.keras.layers.MaxPool3D() #pool_size = (2,2,2)\n",
    "    #         #here\n",
    "    #         tf.keras.layers.Conv3D(kn_1, (5,5,5), activation='relu', padding='same',kernel_regularizer=tf.keras.regularizers.l2(lamda),\n",
    "    # bias_regularizer=tf.keras.regularizers.l1(lamda), activity_regularizer=tf.keras.regularizers.l2(lamda)),\n",
    "    #         tf.keras.layers.MaxPool3D(), #pool_size = (2,2,2)\n",
    "    #         #here\n",
    "            self.Cov_2 =Inception(units=kn_2, strides=1)\n",
    "            # self.MaxPool_2 = tf.keras.layers.MaxPool3D()\n",
    "            # self.Cov_3 = Inception(units=kn_3, strides=1)\n",
    "            self.GlobalMaxPool = tf.keras.layers.GlobalMaxPooling3D()         #tf.keras.layers.Flatten(),\n",
    "            self.Dense_1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "            self.Dense_2 =tf.keras.layers.Dense(32, activation='relu')\n",
    "            self.Dense_3 =tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, input):\n",
    "            x = self.Cov_1(input)\n",
    "            x = self.MaxPool_1(x)\n",
    "            x = self.Cov_2(x) \n",
    "            # x = self.MaxPool_2(x)\n",
    "            # x = self.Cov_3(x)\n",
    "            x = self.GlobalMaxPool(x)\n",
    "            x = self.Dense_1(x)\n",
    "            x = self.Dense_2(x)\n",
    "            x = self.Dense_3(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_5(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "            super().__init__()\n",
    "            \n",
    "            self.Cov_1 = tf.keras.layers.Conv3D(kn_1, ks_fir, activation='relu', padding='same',kernel_regularizer=tf.keras.regularizers.l2(lamda))\n",
    "            self.MaxPool_1 = tf.keras.layers.MaxPool3D() #pool_size = (2,2,2)\n",
    "            #here\n",
    "            self.Cov_2 = tf.keras.layers.Conv3D(kn_1, (5,5,5), activation='relu', padding='same',kernel_regularizer=tf.keras.regularizers.l2(lamda))\n",
    "            self.MaxPool_2 = tf.keras.layers.MaxPool3D() #pool_size = (2,2,2)\n",
    "            #here\n",
    "            self.Cov_3 =Inception(units=kn_2, strides=1)\n",
    "            self.MaxPool_3 = tf.keras.layers.MaxPool3D()\n",
    "            self.Cov_4 = Inception(units=kn_3, strides=1)\n",
    "            self.GlobalMaxPool = tf.keras.layers.GlobalMaxPooling3D()         #tf.keras.layers.Flatten(),\n",
    "            self.Dense_1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "            self.Dense_2 =tf.keras.layers.Dense(32, activation='relu')\n",
    "            self.Dense_3 =tf.keras.layers.Dense(1)\n",
    "            \n",
    "    \n",
    "    def call(self, input):\n",
    "            x = self.Cov_1(input)\n",
    "            x = self.MaxPool_1(x)\n",
    "            #here\n",
    "            x = self.Cov_2(x)\n",
    "            x = self.MaxPool_2(x)\n",
    "            #here\n",
    "            x = self.Cov_3(x) \n",
    "            x = self.MaxPool_3(x)\n",
    "            x = self.Cov_4(x)\n",
    "            x = self.GlobalMaxPool(x)\n",
    "            x = self.Dense_1(x)\n",
    "            x = self.Dense_2(x)\n",
    "            x = self.Dense_3(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Allele_ls = ['HLA-A_0301',\n",
    "            'HLA-B_3501',\n",
    "            'HLA-A_0206',\n",
    "            'HLA-A_3002',\n",
    "            'HLA-A_0205',\n",
    "            'HLA-A_0202',\n",
    "            'HLA-B_2705',\n",
    "            'HLA-A_3001',\n",
    "            'HLA-B_3801',\n",
    "            'HLA-A_3201',\n",
    "            'HLA-B_1501',\n",
    "            'HLA-B_1503']  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从MHCflurry训练验证集提取出可使用的数据集。allele['HLA-B_2704','HLA-B_2706','HLA-B_3801']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HLA-A_0301_(lr_0.0001-bs_32-lam_0.0001-ks_[(3, 3, 27),(1, 3, 5)]-kn_(24, 16, 32)_md_<__main__.Model_1 object at 0x7f4f7e7a6940>)_\n",
      " epoch: 0001, loss: 0.0639 - val_loss: 0.0478; rmse: 0.2475 - rmse_val: 0.2487;  r2: 0.1126 - r2_val: 0.1267; mae: 0.2126 - mae_val: 0.2137; r: 0.4343 - r_val: 0.4863;tau: 0.3219 - tau_val: 0.3664                                                                                                    \n",
      " epoch: 0002, loss: 0.0418 - val_loss: 0.0372; rmse: 0.2114 - rmse_val: 0.2089;  r2: 0.3526 - r2_val: 0.3842; mae: 0.1734 - mae_val: 0.1750; r: 0.6182 - r_val: 0.6499;tau: 0.4705 - tau_val: 0.5049                                                                                                    \n",
      " epoch: 0003, loss: 0.0351 - val_loss: 0.0338; rmse: 0.1972 - rmse_val: 0.1982;  r2: 0.4368 - r2_val: 0.4454; mae: 0.1523 - mae_val: 0.1558; r: 0.6727 - r_val: 0.6783;tau: 0.5073 - tau_val: 0.5296                                                                                                    \n",
      " epoch: 0004, loss: 0.0321 - val_loss: 0.0322; rmse: 0.1886 - rmse_val: 0.1956;  r2: 0.4848 - r2_val: 0.4600; mae: 0.1388 - mae_val: 0.1445; r: 0.7125 - r_val: 0.6935;tau: 0.5369 - tau_val: 0.5407                                                                                                    \n",
      " epoch: 0005, loss: 0.0300 - val_loss: 0.0303; rmse: 0.1801 - rmse_val: 0.1895;  r2: 0.5304 - r2_val: 0.4932; mae: 0.1316 - mae_val: 0.1386; r: 0.7388 - r_val: 0.7124;tau: 0.5562 - tau_val: 0.5553                                                                                                    \n",
      " epoch: 0006, loss: 0.0282 - val_loss: 0.0285; rmse: 0.1731 - rmse_val: 0.1832;  r2: 0.5658 - r2_val: 0.5261; mae: 0.1281 - mae_val: 0.1348; r: 0.7543 - r_val: 0.7260;tau: 0.5691 - tau_val: 0.5654                                                                                                    \n",
      " epoch: 0007, loss: 0.0264 - val_loss: 0.0277; rmse: 0.1677 - rmse_val: 0.1819;  r2: 0.5928 - r2_val: 0.5330; mae: 0.1256 - mae_val: 0.1339; r: 0.7752 - r_val: 0.7324;tau: 0.5859 - tau_val: 0.5687                                                                                                    \n",
      " epoch: 0008, loss: 0.0253 - val_loss: 0.0267; rmse: 0.1609 - rmse_val: 0.1792;  r2: 0.6248 - r2_val: 0.5466; mae: 0.1188 - mae_val: 0.1303; r: 0.7931 - r_val: 0.7396;tau: 0.5996 - tau_val: 0.5725                                                                                                    \n",
      " epoch: 0009, loss: 0.0236 - val_loss: 0.0259; rmse: 0.1605 - rmse_val: 0.1776;  r2: 0.6270 - r2_val: 0.5547; mae: 0.1199 - mae_val: 0.1270; r: 0.8058 - r_val: 0.7551;tau: 0.6097 - tau_val: 0.5862                                                                                                    \n",
      " epoch: 0010, loss: 0.0226 - val_loss: 0.0252; rmse: 0.1515 - rmse_val: 0.1766;  r2: 0.6677 - r2_val: 0.5598; mae: 0.1087 - mae_val: 0.1253; r: 0.8262 - r_val: 0.7590;tau: 0.6251 - tau_val: 0.5844                                                                                                    \n",
      " epoch: 0011, loss: 0.0212 - val_loss: 0.0241; rmse: 0.1465 - rmse_val: 0.1720;  r2: 0.6890 - r2_val: 0.5822; mae: 0.1082 - mae_val: 0.1223; r: 0.8375 - r_val: 0.7667;tau: 0.6371 - tau_val: 0.5913                                                                                                    \n",
      " epoch: 0012, loss: 0.0201 - val_loss: 0.0236; rmse: 0.1396 - rmse_val: 0.1707;  r2: 0.7178 - r2_val: 0.5889; mae: 0.1022 - mae_val: 0.1208; r: 0.8524 - r_val: 0.7696;tau: 0.6510 - tau_val: 0.5923                                                                                                    \n",
      " epoch: 0013, loss: 0.0188 - val_loss: 0.0232; rmse: 0.1369 - rmse_val: 0.1705;  r2: 0.7287 - r2_val: 0.5898; mae: 0.0998 - mae_val: 0.1193; r: 0.8618 - r_val: 0.7746;tau: 0.6609 - tau_val: 0.5961                                                                                                    \n",
      " epoch: 0014, loss: 0.0176 - val_loss: 0.0236; rmse: 0.1340 - rmse_val: 0.1751;  r2: 0.7397 - r2_val: 0.5674; mae: 0.0946 - mae_val: 0.1209; r: 0.8871 - r_val: 0.7819;tau: 0.6822 - tau_val: 0.6030                                                                                                    \n",
      " epoch: 0015, loss: 0.0166 - val_loss: 0.0220; rmse: 0.1221 - rmse_val: 0.1664;  r2: 0.7839 - r2_val: 0.6093; mae: 0.0879 - mae_val: 0.1145; r: 0.8890 - r_val: 0.7843;tau: 0.6858 - tau_val: 0.6034                                                                                                    \n",
      " epoch: 0016, loss: 0.0158 - val_loss: 0.0221; rmse: 0.1126 - rmse_val: 0.1689;  r2: 0.8165 - r2_val: 0.5974; mae: 0.0809 - mae_val: 0.1177; r: 0.9065 - r_val: 0.7742;tau: 0.7048 - tau_val: 0.5930                                                                                                    \n",
      " epoch: 0017, loss: 0.0148 - val_loss: 0.0227; rmse: 0.1135 - rmse_val: 0.1736;  r2: 0.8134 - r2_val: 0.5748; mae: 0.0801 - mae_val: 0.1190; r: 0.9182 - r_val: 0.7736;tau: 0.7190 - tau_val: 0.5952                                                                                                    \n",
      " epoch: 0018, loss: 0.0139 - val_loss: 0.0227; rmse: 0.1116 - rmse_val: 0.1746;  r2: 0.8195 - r2_val: 0.5697; mae: 0.0850 - mae_val: 0.1260; r: 0.9217 - r_val: 0.7704;tau: 0.7259 - tau_val: 0.5895                                                                                                    \n",
      " epoch: 0019, loss: 0.0130 - val_loss: 0.0215; rmse: 0.0960 - rmse_val: 0.1686;  r2: 0.8664 - r2_val: 0.5989; mae: 0.0682 - mae_val: 0.1160; r: 0.9373 - r_val: 0.7816;tau: 0.7446 - tau_val: 0.5968                                                                                                    \n",
      " epoch: 0020, loss: 0.0129 - val_loss: 0.0213; rmse: 0.0888 - rmse_val: 0.1685;  r2: 0.8857 - r2_val: 0.5992; mae: 0.0630 - mae_val: 0.1163; r: 0.9425 - r_val: 0.7771;tau: 0.7515 - tau_val: 0.5947                                                                                                    \n",
      " epoch: 0021, loss: 0.0116 - val_loss: 0.0221; rmse: 0.1008 - rmse_val: 0.1741;  r2: 0.8528 - r2_val: 0.5722; mae: 0.0791 - mae_val: 0.1256; r: 0.9464 - r_val: 0.7801;tau: 0.7603 - tau_val: 0.5955                                                                                                    \n",
      " epoch: 0022, loss: 0.0109 - val_loss: 0.0216; rmse: 0.0795 - rmse_val: 0.1725;  r2: 0.9084 - r2_val: 0.5798; mae: 0.0563 - mae_val: 0.1176; r: 0.9585 - r_val: 0.7701;tau: 0.7806 - tau_val: 0.5881                                                                                                    \n",
      " epoch: 0023, loss: 0.0103 - val_loss: 0.0216; rmse: 0.0716 - rmse_val: 0.1731;  r2: 0.9257 - r2_val: 0.5772; mae: 0.0514 - mae_val: 0.1195; r: 0.9629 - r_val: 0.7657;tau: 0.7891 - tau_val: 0.5869                                                                                                    \n",
      " epoch: 0024, loss: 0.0096 - val_loss: 0.0207; rmse: 0.0649 - rmse_val: 0.1685;  r2: 0.9389 - r2_val: 0.5990; mae: 0.0474 - mae_val: 0.1170; r: 0.9704 - r_val: 0.7753;tau: 0.8069 - tau_val: 0.5885                                                                                                    \n",
      " epoch: 0025, loss: 0.0093 - val_loss: 0.0216; rmse: 0.0681 - rmse_val: 0.1752;  r2: 0.9328 - r2_val: 0.5669; mae: 0.0491 - mae_val: 0.1201; r: 0.9749 - r_val: 0.7624;tau: 0.8165 - tau_val: 0.5766                                                                                                    \n",
      " epoch: 0026, loss: 0.0086 - val_loss: 0.0208; rmse: 0.0544 - rmse_val: 0.1711;  r2: 0.9572 - r2_val: 0.5868; mae: 0.0393 - mae_val: 0.1179; r: 0.9788 - r_val: 0.7700;tau: 0.8267 - tau_val: 0.5875                                                                                                    \n",
      " epoch: 0027, loss: 0.0082 - val_loss: 0.0216; rmse: 0.0567 - rmse_val: 0.1766;  r2: 0.9534 - r2_val: 0.5598; mae: 0.0412 - mae_val: 0.1205; r: 0.9816 - r_val: 0.7621;tau: 0.8373 - tau_val: 0.5754                                                                                                    \n",
      " epoch: 0028, loss: 0.0078 - val_loss: 0.0211; rmse: 0.0555 - rmse_val: 0.1745;  r2: 0.9554 - r2_val: 0.5704; mae: 0.0419 - mae_val: 0.1207; r: 0.9828 - r_val: 0.7672;tau: 0.8385 - tau_val: 0.5849                                                                                                    \n",
      " epoch: 0029, loss: 0.0074 - val_loss: 0.0211; rmse: 0.0433 - rmse_val: 0.1757;  r2: 0.9728 - r2_val: 0.5644; mae: 0.0311 - mae_val: 0.1207; r: 0.9865 - r_val: 0.7580;tau: 0.8527 - tau_val: 0.5775                                                                                                    \n",
      " epoch: 0030, loss: 0.0070 - val_loss: 0.0208; rmse: 0.0388 - rmse_val: 0.1744;  r2: 0.9782 - r2_val: 0.5705; mae: 0.0283 - mae_val: 0.1209; r: 0.9894 - r_val: 0.7601;tau: 0.8621 - tau_val: 0.5771                                                                                                    \n",
      " epoch: 0031, loss: 0.0067 - val_loss: 0.0211; rmse: 0.0427 - rmse_val: 0.1771;  r2: 0.9736 - r2_val: 0.5574; mae: 0.0320 - mae_val: 0.1221; r: 0.9914 - r_val: 0.7598;tau: 0.8714 - tau_val: 0.5703                                                                                                    \n",
      " epoch: 0032, loss: 0.0065 - val_loss: 0.0207; rmse: 0.0329 - rmse_val: 0.1757;  r2: 0.9843 - r2_val: 0.5642; mae: 0.0242 - mae_val: 0.1215; r: 0.9923 - r_val: 0.7568;tau: 0.8765 - tau_val: 0.5727                                                                                                    \n",
      " epoch: 0033, loss: 0.0062 - val_loss: 0.0205; rmse: 0.0310 - rmse_val: 0.1754;  r2: 0.9861 - r2_val: 0.5659; mae: 0.0230 - mae_val: 0.1214; r: 0.9933 - r_val: 0.7589;tau: 0.8810 - tau_val: 0.5752                                                                                                    \n",
      " epoch: 0034, loss: 0.0060 - val_loss: 0.0201; rmse: 0.0320 - rmse_val: 0.1741;  r2: 0.9852 - r2_val: 0.5720; mae: 0.0246 - mae_val: 0.1220; r: 0.9939 - r_val: 0.7597;tau: 0.8860 - tau_val: 0.5731                                                                                                    \n"
     ]
    }
   ],
   "source": [
    "class_name = 'class_1'                                                     #here!\n",
    "# Train_dir = '/raid/hly/PK-3[2018PLOS]/data/train_data-MID/'+ class_name +'_pre'     \n",
    "Train_dir = '/raid/hly/PK-4[MHCflurry]/no_mass_spec/data/trainval_MID_otherAllele'\n",
    "Test_dir = '/mnt/zt/Dopaap/calculate_webtools_score/IEDB_new_released_dataset/iedb_tools_predict_data_processed'\n",
    "\n",
    "for Allele in os.listdir(Test_dir):\n",
    "# for Allele in Allele_ls:\n",
    "    if Allele not in Allele_ls:\n",
    "        continue\n",
    "    df_test = pd.read_csv(os.path.join(Test_dir, Allele, 'tools_processed.csv'))  \n",
    "    \n",
    "    # 创建储存 Allele 结果的文件夹\n",
    "    task_dir = '/mnt/zt/Dopaap/dopaap_predictor/PK-3_2018PLOS_from_hly_vaccin_task/loss_models_results'             \n",
    "    Allele_fold = os.path.join(task_dir, Allele)\n",
    "\n",
    "    if not os.path.exists(Allele_fold) : \n",
    "        os.makedirs(Allele_fold)\n",
    "        os.makedirs(os.path.join(Allele_fold,'loss'))\n",
    "        os.makedirs(os.path.join(Allele_fold,'models'))\n",
    "        os.makedirs(os.path.join(Allele_fold,'results'))\n",
    "        # os.makedirs(os.path.join(Allele_fold,'fig-loss'))\n",
    "        # os.makedirs(os.path.join(Allele_fold,'fig-pfm'))\n",
    "        # os.makedirs(os.path.join(Allele_fold,'fig-pred_true')) \n",
    "\n",
    "    # 获得测试集的最大序列长度\n",
    "    max_seq_len = test_max_seq_len = df_test['Description'].apply(len).max()\n",
    "   \n",
    "    X_test_data_dir = '/mnt/zt/Dopaap/dopaap_predictor/PK-3_2018PLOS_from_hly_vaccin_task/X_test_data'\n",
    "   \n",
    "   # 生成 X_test + Y_test \n",
    "    X_test_name = os.path.join(X_test_data_dir, Allele+'_X_test_'+'.data')\n",
    "    if not os.path.exists(X_test_name) :\n",
    "        X_test = []\n",
    "        for seq in df_test['Description']:\n",
    "            X_test.append(get_3d_feat(seq))\n",
    "        X_test = np.stack(X_test)\n",
    "        dump(X_test, X_test_name)\n",
    "    else:\n",
    "        X_test = load(X_test_name)\n",
    "    X_test = X_test.astype('float32')\n",
    "    Y_test = df_test['Normalized_QM'].values.reshape(-1, 1) \n",
    "\n",
    "    # 确定参数\n",
    "    ## 公共参数\n",
    "    inc = (1,3,5)\n",
    "    lr = 1e-4\n",
    "    bs = 32\n",
    "    \n",
    "    ks_fir_ls = [(3,3,27),(3,3,7)] \n",
    "    kn_ls = [(24,16,32),(9,6,12),(48,32,64)] \n",
    "    lamda_ls = [1e-4,1e-6] \n",
    "    #******************************************\n",
    "    task_name = Allele +'_noMS'\n",
    "\n",
    "    for ks_fir in ks_fir_ls:\n",
    "        for kn in kn_ls:\n",
    "            kn_1,kn_2,kn_3 = kn[0],kn[1],kn[2]\n",
    "            for lamda in lamda_ls:\n",
    "                model_ls = [Model_1(),Model_2(),Model_3(),Model_4()]\n",
    "                for model in model_ls: \n",
    "\n",
    "                    # continue 情况\n",
    "                    df_done = pd.read_csv(os.path.join(task_dir,'done_list.csv'))\n",
    "                    exists_name = Allele + '_(lr_%s-bs_%s-lam_%s-ks_[%s,%s]-kn_%s_md_%s)_' %(lr,bs,lamda,ks_fir,inc,kn,str(model))\n",
    "                    if (exists_name +'_results.csv') in (df_done['done_file'].values):\n",
    "                        continue\n",
    "\n",
    "                    # 查看目前最佳 R2\n",
    "                    result_file = os.path.join(Allele_fold,'results')\n",
    "                    result_csv = result_file + '/' + task_name + '_results.csv'\n",
    "                    if os.path.exists(result_csv) :\n",
    "                        df_exit_res = pd.read_csv(result_csv)\n",
    "                        best_exit_r2 = df_exit_res['test_r2'][0]\n",
    "                    else:\n",
    "                        best_exit_r2 = -1000\n",
    "\n",
    "                    for n in range(1):\n",
    "                \n",
    "                        print(exists_name)  \n",
    "                        lr = lr\n",
    "                        patience = 50\n",
    "                        epochs = 300\n",
    "                        loss= tf.keras.losses.log_cosh     #weighted_loss #tf.keras.losses.mean_squared_error \n",
    "                        batch_size = bs\n",
    "\n",
    "                        df_loss = pd.DataFrame()          \n",
    "                        results = []\n",
    "\n",
    "                        opt = tf.keras.optimizers.Adam(lr=lr)\n",
    "                        model.compile(optimizer=opt, loss=loss)\n",
    "\n",
    "                        performance = molmap.model.cbks.Reg3D_EarlyStoppingAndPerformance((X_train, Y_train), \n",
    "                                                                        (X_valid, Y_valid), \n",
    "                                                                    patience = patience,\n",
    "                                                                    )\n",
    "\n",
    "                        model.fit(X_train, Y_train, batch_size=batch_size, \n",
    "                                epochs = epochs, verbose = 0, shuffle = True, \n",
    "                                validation_data = (X_valid, Y_valid), callbacks = [performance]) \n",
    "                    \n",
    "                        best_epoch = performance.best_epoch\n",
    "                        trainable_params = model.count_params()\n",
    "\n",
    "                        #获取RMSE和R2\n",
    "                        train_rmse,train_mae,train_r,train_r2,train_tau = performance.evaluate(X_train, Y_train)            \n",
    "                        valid_rmse,valid_mae,valid_r,valid_r2,valid_tau = performance.evaluate(X_valid, Y_valid)\n",
    "\n",
    "                        # make prediction\n",
    "                        Y_test_pred = model.predict(X_test)\n",
    "                        df_pred = pd.DataFrame(Y_test_pred.tolist()).rename(columns={0:'Pred_Norm_QM'})\n",
    "                        df_pred['Pred_QM'] = df_pred['Pred_Norm_QM'].apply(to_ic50)\n",
    "                        df_truepred = pd.merge(df_test,df_pred,how='inner',left_index=True,right_index=True)   \n",
    "\n",
    "                        y_true = df_truepred['Normalized_QM']\n",
    "                        y_pred = df_truepred['Pred_Norm_QM']\n",
    "                        test_r2 = r2_score(y_true, y_pred)   \n",
    "                        test_r = PCC(y_true, y_pred)           \n",
    "        \n",
    "                        if test_r2 >= best_exit_r2 :\n",
    "\n",
    "                            #储存loss\n",
    "                            dfl = pd.DataFrame(performance.history)\n",
    "                            df_loss = df_loss.append(dfl, ignore_index = True)\n",
    "                            df_loss.to_csv(os.path.join(os.path.join(Allele_fold,'loss'),  task_name +'_loss.csv'))\n",
    "                        \n",
    "                            # 储存 prediction\n",
    "                            df_truepred.to_csv(os.path.join(Allele_fold,Allele+'true_pred.csv'))\n",
    "\n",
    "                            # 计算 tau, auc, f1\n",
    "                            test_tau = scipy.stats.kendalltau(y_pred, y_true)[0]\n",
    "\n",
    "                            y_pred_score = from_ic50(y_pred, max_ic50)\n",
    "                            try:\n",
    "                                test_auc = sklearn.metrics.roc_auc_score(\n",
    "                                            y_true <= threshold_nm,\n",
    "                                            y_pred_score,\n",
    "                                            sample_weight=sample_weight)\n",
    "                            except ValueError as e:\n",
    "                                logging.warning(e)\n",
    "                                test_auc = np.nan\n",
    "                            \n",
    "                            try:\n",
    "                                test_f1 = sklearn.metrics.f1_score(\n",
    "                                            y_true <= threshold_nm,\n",
    "                                            y_pred <= threshold_nm,\n",
    "                                            sample_weight=sample_weight)\n",
    "                            except ValueError as e:\n",
    "                                logging.warning(e)\n",
    "                                test_f1 = np.nan\n",
    "\n",
    "                            #整体性结果\n",
    "                            final_res = {\n",
    "                                        'train_rmse':np.nanmean(train_rmse), \n",
    "                                        'valid_rmse':np.nanmean(valid_rmse),  \n",
    "                                        # 'train_mse':np.nanmean(train_mse), \n",
    "                                        # 'valid_mse':np.nanmean(test_mse),                      \n",
    "                                        'train_r2':np.nanmean(train_r2), \n",
    "                                        'valid_r2':np.nanmean(valid_r2), \n",
    "                                        'train_mae':np.nanmean(train_mae), \n",
    "                                        'valid_mae':np.nanmean(valid_mae),\n",
    "                                        'train_r':np.nanmean(train_r), \n",
    "                                        'valid_r':np.nanmean(valid_r),\n",
    "                                        'train_tau':np.nanmean(train_tau), \n",
    "                                        'valid_tau':np.nanmean(valid_tau),\n",
    "                                        # 'train_auc':np.nanmean(train_auc), \n",
    "                                        # 'valid_auc':np.nanmean(valid_auc),\n",
    "                                        'test_r2':np.nanmean(test_r2),\n",
    "                                        'test_r':np.nanmean(test_r),\n",
    "                                        'test_tau':np.nanmean(test_tau),\n",
    "                                        'test_auc':np.nanmean(test_auc),\n",
    "                                        'test_f1' : np.nanmean(test_f1),\n",
    "                                        'trainable params': trainable_params, \n",
    "                                        'best_epoch': best_epoch,\n",
    "                                        'lr' : lr,\n",
    "                                        'batch_size':bs,\n",
    "                                        'kernel_size_1':ks_fir,\n",
    "                                        'kernel_size_incept': inc,\n",
    "                                        'kernel_number':kn,\n",
    "                                        'lamda' : lamda,\n",
    "                                        'model' : model\n",
    "                                        }\n",
    "\n",
    "                            results.append(final_res)\n",
    "                            dfr = pd.DataFrame(results)\n",
    "                            dfr.to_csv(os.path.join(os.path.join(Allele_fold,'results'),  task_name +'_results.csv'))\n",
    "                            \n",
    "                            # 保存模型\n",
    "                            model.save_weights(os.path.join(os.path.join(Allele_fold,'models'),  task_name +'_model_'+'.h5'))\n",
    "                            \n",
    "                            # 删除model\n",
    "                            del model\n",
    "\n",
    "                        print('*******************************************************')\n",
    "                \n",
    "\n",
    "                    df_done = pd.read_csv(os.path.join(task_dir,'done_list.csv'))  \n",
    "                    new_donels = df_done['done_file'].append(pd.Series(exists_name +'_results.csv'))\n",
    "                    df_newdone = pd.DataFrame(new_donels,columns=['done_file'])\n",
    "                    df_newdone.to_csv(os.path.join(task_dir,'done_list.csv'))\n",
    "    \n",
    "    \n",
    "    # # 删除缓存\n",
    "    # del performance\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    tf.compat.v1.reset_default_graph() # TF graph isn't same as Keras grap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HLA-B_1501'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Allele"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaccin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "616f56fef8ea7183d51ad2039e0213a0ade719c1a48fd338f6185e7423826c67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
