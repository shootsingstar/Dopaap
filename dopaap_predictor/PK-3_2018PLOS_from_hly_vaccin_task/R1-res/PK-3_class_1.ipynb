{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAABICAYAAADI6S+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAACDElEQVR4nO3aP2pUURjG4e8ElWhs/BPstBIUFBQHtyK4gNmQnQtwCZItCFEbVyFRiKCVcGxsHNRhYI7vzPF5ulxu8X4EfsVlWu+9APj3DtIDAP5XAgwQIsAAIQIMECLAACECDBByYd0LrbVlVS2rqo4uHjy5d+3y8FEp368fpScM9eXKvP+785r3tqqq82+X0hOGOvy8NkV77evHd2e99+PV522T3wEvbl3tp88ebnXYLvn0/Gl6wlAnjx+lJwzzuu6nJwx18v52esJQd1/dSE8Y6s2Lw7e998Xqc58gAEIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggpPXe//5Ca8uqWv7880FVfRg9KuhmVZ2lRwwy821V7tt3s993p/d+vPpwbYB/ebm10977YquzdsjM9818W5X79t3s9/2JTxAAIQIMELJpgF8OWbE7Zr5v5tuq3LfvZr/vtzb6BgzA9vgEARAiwAAhAgwQIsAAIQIMEPID0IJRG96Z/ToAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'\n",
    "\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import molmap.model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette(\"rainbow_r\", 6) #PiYG\n",
    "sns.palplot(color)\n",
    "\n",
    "from joblib import load, dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AA_pro\n",
    "PCP = pd.read_csv('/raid/hly/vaccin/data/cal_CTD/PCP_61.csv', index_col='properties').index\n",
    "AApro_dic = pd.read_csv('/raid/hly/umap/single_index/AApro_UMAP.csv', header=0,index_col=0)\n",
    "AA_pro = AApro_dic.loc[PCP].astype('float')\n",
    "AA_pro = AA_pro.T\n",
    "AA_pro = ((AA_pro - AA_pro.min()) / (AA_pro.max() - AA_pro.min())).T #归一化\n",
    "\n",
    "\n",
    "# get_3d_feat -- our method (老师筛选了31个理化性质，一个aa对应一个值)\n",
    "def get_3d_feat2(seq):\n",
    "    n = len(seq)\n",
    "    # 生成理化性质矩阵\n",
    "    seq_pro = pd.DataFrame(AA_pro[aa] for aa in seq).T\n",
    "    seq_pro = seq_pro.values[:, :, None]  #二维变成三维（31， n， 1）\n",
    "    # 相乘开根号\n",
    "    mt_pro = np.transpose((seq_pro * np.transpose(seq_pro, [0, 2, 1])), [1, 2, 0]) **.25\n",
    "    #transpose三维转置。（31，n，1）*（31，1，n）=（31，n，n），再转置成（n，n，31）.\n",
    "    for k in range(mt_pro.shape[2]):\n",
    "        for i in range(n):\n",
    "            for j in range(i):\n",
    "                if k < 60:                                      ##here\n",
    "                    mt_pro[i,j,k] = (mt_pro[j,i,k] * mt_pro[j,i,k+1])**.5\n",
    "                else:\n",
    "                    mt_pro[i,j,k] = (mt_pro[j,i,k] * mt_pro[j,i,0])**.5\n",
    "                \n",
    "    # 生成序列距离矩阵\n",
    "    pt_dis = np.ones((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            pt_dis[i][j] = abs(i-j)\n",
    "    pt_dis = ((pt_dis - 0) / (n-1 - 0)) ## 归一化（最小值为0，最大值为n-1）\n",
    "    pt_dis = pt_dis[:, :, np.newaxis]  ##（n，n，1）\n",
    "\n",
    "    # 将序列距离矩阵与理化性质矩阵合并，并放在第一层\n",
    "    mt = np.concatenate((pt_dis,mt_pro),axis = 2)\n",
    "    \n",
    "    x = np.pad(mt, [(0, max_seq_len-n), (0, max_seq_len-n), (0, 0)]) #填充0.令数据集中每条序列对应的Xshape相同。\n",
    "\n",
    "    return x[ :, :, :, None]\n",
    "\n",
    "\n",
    "def get_pos_weight(c):\n",
    "    cnt = [0] * 8\n",
    "    for i in Y_train:\n",
    "        if 2<= i < 4:\n",
    "            cnt[0] = cnt[0] + 1\n",
    "        if 4<= i < 5:\n",
    "            cnt[1] = cnt[1] + 1\n",
    "        if 5 <= i < 6:\n",
    "            cnt[2] = cnt[2] + 1\n",
    "        if 6<= i < 7:\n",
    "            cnt[3] = cnt[3] + 1\n",
    "        if 7<= i < 8:\n",
    "            cnt[4] = cnt[4] + 1\n",
    "        if 8<= i < 9:\n",
    "            cnt[5] = cnt[5] + 1\n",
    "        if 9<= i < 10:\n",
    "            cnt[6] = cnt[6] + 1\n",
    "        if 10<= i < 12:\n",
    "            cnt[7] = cnt[7] + 1\n",
    "    total = sum(cnt)\n",
    "    for i in range(len(cnt)):\n",
    "        if cnt[i] != 0:\n",
    "            cnt[i] = (c * total) / (cnt[i] + c * total)\n",
    "    return np.array(cnt)\n",
    "\n",
    "\n",
    "class Inception(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units = 8, strides = 1):\n",
    "        super(Inception, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv3D(units, (1,1,1), padding='same', activation = 'relu', strides = strides,kernel_regularizer=tf.keras.regularizers.l2(lamda),\n",
    "  bias_regularizer=tf.keras.regularizers.l1(lamda), activity_regularizer=tf.keras.regularizers.l2(lamda))\n",
    "        self.conv2 = tf.keras.layers.Conv3D(units, (3,3,3), padding='same', activation = 'relu', strides = strides,kernel_regularizer=tf.keras.regularizers.l2(lamda),\n",
    "  bias_regularizer=tf.keras.regularizers.l1(lamda), activity_regularizer=tf.keras.regularizers.l2(lamda))\n",
    "        self.conv3 = tf.keras.layers.Conv3D(units, (5,5,5), padding='same', activation = 'relu', strides = strides,kernel_regularizer=tf.keras.regularizers.l2(lamda),\n",
    "  bias_regularizer=tf.keras.regularizers.l1(lamda), activity_regularizer=tf.keras.regularizers.l2(lamda))\n",
    "        self.concat = tf.keras.layers.Concatenate()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x1 = self.conv1(inputs)\n",
    "        x2 = self.conv2(inputs)\n",
    "        x3 = self.conv3(inputs)\n",
    "        outputs = self.concat([x1, x2, x3])\n",
    "        return outputs\n",
    "\n",
    "    def get_config(self): \n",
    "        config = {\"conv1\": self.conv1,\"conv2\":self.conv2,'conv3':self.conv3}\n",
    "        base_config = super(Inception, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = 'class_1'                                                     #here!\n",
    "Train_dir = '/raid/hly/PK-3[2018PLOS]/data/train_data-MID/'+ class_name      \n",
    "Test_dir = '/raid/hly/PK-3[2018PLOS]/data/PK-3_benchmark_data'\n",
    "\n",
    "for file in os.listdir(Train_dir):\n",
    "    \n",
    "    Allele = file.split('.')[0]\n",
    "    df_train = pd.read_csv(os.path.join(Train_dir,file))\n",
    "    df_test = pd.read_csv(os.path.join(Test_dir,file))  \n",
    "    \n",
    "    # 创建储存 Allele 结果的文件夹\n",
    "    res_class = '/raid/hly/PK-3[2018PLOS]/R1-res/'+ class_name               \n",
    "    Allele_fold = os.path.join(res_class,Allele)\n",
    "\n",
    "    if not os.path.exists(Allele_fold) : \n",
    "        os.makedirs(Allele_fold)\n",
    "        os.makedirs(os.path.join(Allele_fold,'loss'))\n",
    "        os.makedirs(os.path.join(Allele_fold,'models'))\n",
    "        os.makedirs(os.path.join(Allele_fold,'results'))\n",
    "        # os.makedirs(os.path.join(Allele_fold,'fig-loss'))\n",
    "        # os.makedirs(os.path.join(Allele_fold,'fig-pfm'))\n",
    "        # os.makedirs(os.path.join(Allele_fold,'fig-pred_true')) \n",
    "\n",
    "    # 获得训练集和测试集的最大序列长度\n",
    "    train_max_seq_len = df_train['Description'].apply(len).max()\n",
    "    test_max_seq_len = df_test['Description'].apply(len).max()\n",
    "    max_seq_len = max(train_max_seq_len,test_max_seq_len)\n",
    "\n",
    "\n",
    "    # 生成 X_train + Y_train \n",
    "    X_train_name = os.path.join(Allele_fold, Allele+'_X_train_'+'.data')\n",
    "    if not os.path.exists(X_train_name) :\n",
    "        X_train = []\n",
    "        for seq in df_train['Description']:\n",
    "            X_train.append(get_3d_feat2(seq))\n",
    "        X_train = np.stack(X_train)\n",
    "        dump(X_train, X_train_name)\n",
    "    else:\n",
    "        X_train = load(X_train_name)\n",
    "    X_train = X_train.astype('float32')\n",
    "    Y_train = df_train['Normalized_QM'].values.reshape(-1, 1) \n",
    "   \n",
    "   # 生成 X_test + Y_test \n",
    "    X_test_name = os.path.join(Allele_fold, Allele+'_X_test_'+'.data')\n",
    "    if not os.path.exists(X_test_name) :\n",
    "        X_test = []\n",
    "        for seq in df_test['Description']:\n",
    "            X_test.append(get_3d_feat2(seq))\n",
    "        X_test = np.stack(X_test)\n",
    "        dump(X_test, X_test_name)\n",
    "    else:\n",
    "        X_test = load(X_test_name)\n",
    "    X_test = X_test.astype('float32')\n",
    "    Y_test = df_test['Normalized_QM'].values.reshape(-1, 1) \n",
    "\n",
    "    # 确定参数\n",
    "    inc = (1,3,5)\n",
    "    lr = 1e-4\n",
    "    ks_fir_ls = [(3,3,7),(5,5,9),(9,9,13)]\n",
    "    kn = (48,32,64)\n",
    "    kn_1,kn_2,kn_3 = kn[0],kn[1],kn[2]\n",
    "    lamda_ls = [1e-3,1e-6,1e-9]  \n",
    "    bs = 32\n",
    "   \n",
    "   #******************************************\n",
    "    R2_ls = []\n",
    "    task_name = Allele +'ksfir_lamda'\n",
    "\n",
    "    for ks_fir in ks_fir_ls:\n",
    "        for lamda in lamda_ls:\n",
    "\n",
    "            # continue 情况\n",
    "            df_done = pd.read_csv('/raid/hly/PK-3[2018PLOS]/R1-res/done_list.csv')\n",
    "            exists_name = Allele + '_(lr_%s-bs_%s-lam_%s-ks_[%s,%s]-kn_%s)_' %(lr,bs,lamda,ks_fir,inc,kn)\n",
    "            if (exists_name +'_results.csv') in (df_done['done_file'].values):\n",
    "                continue\n",
    "\n",
    "            for n in range(2):\n",
    "        \n",
    "                print(exists_name)  \n",
    "                lr = lr\n",
    "                patience = 100\n",
    "                epochs = 600\n",
    "                loss= tf.keras.losses.log_cosh     #weighted_loss #tf.keras.losses.mean_squared_error \n",
    "                batch_size = bs\n",
    "\n",
    "                df_loss = pd.DataFrame()          \n",
    "                results = []\n",
    "\n",
    "\n",
    "                model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Conv3D(kn_1, ks_fir, activation='relu', padding='same',kernel_regularizer=tf.keras.regularizers.l2(lamda),\n",
    "        bias_regularizer=tf.keras.regularizers.l1(lamda), activity_regularizer=tf.keras.regularizers.l2(lamda)),\n",
    "                tf.keras.layers.MaxPool3D(), #pool_size = (2,2,2)\n",
    "                Inception(units=kn_2, strides=1),\n",
    "                tf.keras.layers.MaxPool3D(),\n",
    "                Inception(units=kn_3, strides=1),\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(128, activation='relu'),\n",
    "                tf.keras.layers.Dense(32, activation='relu'),\n",
    "                tf.keras.layers.Dense(1)\n",
    "                ])\n",
    "\n",
    "                opt = tf.keras.optimizers.Adam(lr=lr)\n",
    "                model.compile(optimizer=opt, loss=loss)\n",
    "\n",
    "                performance = molmap.model.cbks.Reg3D_EarlyStoppingAndPerformance((X_train, Y_train), \n",
    "                                                                (X_test, Y_test), \n",
    "                                                            patience = patience,\n",
    "                                                            )\n",
    "\n",
    "                model.fit(X_train, Y_train, batch_size=batch_size, \n",
    "                        epochs = epochs, verbose = 0, shuffle = True, \n",
    "                        validation_data = (X_test, Y_test), callbacks = [performance]) \n",
    "\n",
    "\n",
    "                best_epoch = performance.best_epoch\n",
    "                trainable_params = model.count_params()\n",
    "\n",
    "                #获取RMSE和R2\n",
    "                train_rmse,train_mse,train_mae,train_r,train_r2 = performance.evaluate(X_train, Y_train)            \n",
    "                test_rmse,test_mse,test_mae,test_r,test_r2 = performance.evaluate(X_test, Y_test)\n",
    "\n",
    "                R2_ls.append(test_r2)                \n",
    "\n",
    "                if test_r2 >= max(R2_ls) :\n",
    "\n",
    "                    #储存loss\n",
    "                    dfl = pd.DataFrame(performance.history)\n",
    "                    df_loss = df_loss.append(dfl, ignore_index = True)\n",
    "                    df_loss.to_csv(os.path.join(os.path.join(Allele_fold,'loss'),  task_name +'_loss.csv'))\n",
    "\n",
    "                    #整体性结果\n",
    "                    final_res = {\n",
    "                                    'train_rmse':np.nanmean(train_rmse), \n",
    "                                    'valid_rmse':np.nanmean(test_rmse),  \n",
    "                                    'train_mse':np.nanmean(train_mse), \n",
    "                                    'valid_mse':np.nanmean(test_mse),                      \n",
    "                                    'train_r2':np.nanmean(train_r2), \n",
    "                                    'valid_r2':np.nanmean(test_r2), \n",
    "                                    'train_mae':np.nanmean(train_mae), \n",
    "                                    'valid_mae':np.nanmean(test_mae),\n",
    "                                    'train_r':np.nanmean(train_r), \n",
    "                                    'valid_r':np.nanmean(test_r),\n",
    "                                    'trainable params': trainable_params, \n",
    "                                    'best_epoch': best_epoch,\n",
    "                                    'lr' : lr,\n",
    "                                    'batch_size':bs,\n",
    "                                    'kernel_size_1':ks_fir,\n",
    "                                    'kernel_size_incept': inc,\n",
    "                                    'kernel_number':kn,\n",
    "                                    'lamda' : lamda\n",
    "                                    }\n",
    "\n",
    "                    results.append(final_res)\n",
    "                    dfr = pd.DataFrame(results)\n",
    "                    dfr.to_csv(os.path.join(os.path.join(Allele_fold,'results'),  task_name +'_results.csv'))\n",
    "                    \n",
    "                    # 保存模型\n",
    "                    model.save(os.path.join(os.path.join(Allele_fold,'models'),  task_name +'_model_' +'.h5'))\n",
    "\n",
    "                    # make prediction\n",
    "                    Y_test_pred = model.predict(X_test)\n",
    "                    df_pred = pd.DataFrame(Y_test_pred.tolist()).rename(columns={0:'Pred_QM'})\n",
    "                    df_truepred = pd.merge(df_test,df_pred,how='inner',left_index=True,right_index=True)\n",
    "                    df_truepred.to_csv(os.path.join(Allele_fold,Allele+'true_pred.csv'))\n",
    "                    \n",
    "                    # 删除model\n",
    "                    del model\n",
    "\n",
    "                print('*******************************************************')\n",
    "            \n",
    "            df_done = pd.read_csv('/raid/hly/PK-3[2018PLOS]/R1-res/done_list.csv')   \n",
    "            new_donels = df_done['done_file'].append(pd.Series(exists_name +'_results.csv'))\n",
    "            df_newdone = pd.DataFrame(new_donels,columns=['done_file'])\n",
    "            df_newdone.to_csv('/raid/hly/PK-3[2018PLOS]/R1-res/done_list.csv')\n",
    "    \n",
    "    \n",
    "    # # 删除缓存\n",
    "    # del performance\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    tf.compat.v1.reset_default_graph() # TF graph isn't same as Keras grap\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "616f56fef8ea7183d51ad2039e0213a0ade719c1a48fd338f6185e7423826c67"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
