{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.stats\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def PCC_score(y_pred,y_true):\n",
    "    diff_pred,diff_true=y_pred-np.mean(y_pred),y_true-np.mean(y_true)\n",
    "    return np.sum(diff_pred*diff_true)/np.sqrt(np.sum(diff_pred**2)*np.sum(diff_true**2))\n",
    "\n",
    "# MAE = mean_absolute_error(y_true, tool_data_list[i])\n",
    "# MSE = mean_squared_error(y_true, tool_data_list[i])\n",
    "# RMSE = np.sqrt(MSE)\n",
    "# r2 = r2_score(y_true, tool_data_list[i])\n",
    "# PCC = PCC_score(y_true, tool_data_list[i])\n",
    "# tau = scipy.stats.kendalltau(y_true, tool_data_list[i])[0]\n",
    "\n",
    "allele_size_cutoff = 1\n",
    "prediction_dir = '/mnt/zt/Dopaap/calculate_webtools_score/compare_HPV16_dataset/processed_for_calculate'\n",
    "results_score_dir = '/mnt/zt/Dopaap/calculate_webtools_score/compare_HPV16_dataset/results_score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_allele_score():    \n",
    "    df_results_score = pd.DataFrame(columns=['allele','allele_size', 'tool', 'r2', 'PCC', 'tau', 'MAE', 'MSE', 'RMSE'])\n",
    "\n",
    "    for file in os.listdir(prediction_dir):\n",
    "        allele = Path(file).stem\n",
    "        df_allele_prediction = pd.read_csv(os.path.join(prediction_dir, file))\n",
    "\n",
    "        if not df_allele_prediction.columns.str.contains('Dopaap').any():\n",
    "            continue\n",
    "        \n",
    "        if len(df_allele_prediction) < allele_size_cutoff:\n",
    "            continue\n",
    "        \n",
    "        y_true = df_allele_prediction['Normalized_QM']\n",
    "        \n",
    "        y_pred_list = [tool_score for tool_score in df_allele_prediction.columns if '_Normalized' in tool_score]\n",
    "        for tool_score in y_pred_list:\n",
    "            y_pred = df_allele_prediction[tool_score]\n",
    "            MAE = mean_absolute_error(y_true, y_pred)\n",
    "            MSE = mean_squared_error(y_true, y_pred)\n",
    "            RMSE = np.sqrt(MSE)\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            PCC = PCC_score(y_true, y_pred)\n",
    "            tau = scipy.stats.kendalltau(y_true, y_pred)[0]\n",
    "            \n",
    "            tool_name = tool_score.split('_')[0]\n",
    "            df_results_score = df_results_score.append({'allele': allele, 'allele_size': len(df_allele_prediction), 'tool': tool_name, 'r2': r2, 'PCC': PCC, 'tau': tau , 'MAE': MAE, 'MSE': MSE, 'RMSE': RMSE}, ignore_index=True)\n",
    "    df_results_score.to_csv(os.path.join(results_score_dir, 'results_score.csv'), index=False)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_score(): \n",
    "    df_results_score = pd.read_csv(os.path.join(results_score_dir, 'results_score.csv'))\n",
    "    df_total_prediction = pd.DataFrame()\n",
    "\n",
    "    for file in os.listdir(prediction_dir):\n",
    "        allele = Path(file).stem\n",
    "        df_allele_prediction = pd.read_csv(os.path.join(prediction_dir, file))\n",
    "        \n",
    "        if not df_allele_prediction.columns.str.contains('Dopaap').any():\n",
    "            continue\n",
    "        \n",
    "        if len(df_allele_prediction) < allele_size_cutoff:\n",
    "            continue\n",
    "        \n",
    "        df_total_prediction = pd.concat([df_total_prediction, df_allele_prediction])\n",
    "        \n",
    "    df_total_prediction.to_csv(os.path.join(results_score_dir, 'total_prediction.csv'), index=False)\n",
    "\n",
    "    y_pred_list = [tool_score for tool_score in df_total_prediction.columns if '_Normalized' in tool_score]\n",
    "\n",
    "    for tool_score in y_pred_list:\n",
    "        df_tool_prediction = df_total_prediction[~pd.isna(df_total_prediction[tool_score])]\n",
    "        \n",
    "        y_true = df_tool_prediction['Normalized_QM']\n",
    "        y_pred = df_tool_prediction[tool_score]\n",
    "        \n",
    "        MAE = mean_absolute_error(y_true, y_pred)\n",
    "        MSE = mean_squared_error(y_true, y_pred)\n",
    "        RMSE = np.sqrt(MSE)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        PCC = PCC_score(y_true, y_pred)\n",
    "        tau = scipy.stats.kendalltau(y_true, y_pred)[0]\n",
    "        \n",
    "        tool_name = tool_score.split('_')[0]\n",
    "        \n",
    "        df_results_score = df_results_score.append({'allele': 'Total allele', 'allele_size': len(df_tool_prediction), 'tool': tool_name, 'r2': r2, 'PCC': PCC, 'tau': tau , 'MAE': MAE, 'MSE': MSE, 'RMSE': RMSE}, ignore_index=True)\n",
    "    df_results_score.to_csv(os.path.join(results_score_dir, 'results_score.csv'), index=False)        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_allele_score()\n",
    "calculate_total_score()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molmap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
